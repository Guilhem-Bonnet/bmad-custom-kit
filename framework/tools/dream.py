#!/usr/bin/env python3
"""
dream.py ‚Äî BMAD Dream Mode : consolidation hors-session et insights √©mergents.
==============================================================================

Simule une phase de "r√™ve" : les agents relisent learnings, decisions, trace,
failure museum et shared-context, puis produisent des insights cross-domaine
qu'aucun agent n'aurait formul√©s en session.

Mode read-only : aucun fichier n'est modifi√©. Les insights sont √©crits dans
_bmad-output/dream-journal.md pour review humain.

Usage :
  python3 dream.py --project-root .                   # Dream complet
  python3 dream.py --project-root . --since 2026-01-01 # Depuis une date
  python3 dream.py --project-root . --agent dev        # Focus un agent
  python3 dream.py --project-root . --validate         # Valider les insights (no hallucination)
  python3 dream.py --project-root . --dry-run          # Preview sans √©crire

Stdlib only ‚Äî aucune d√©pendance externe.
"""

import argparse
import json
import re
import sys
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path

# ‚îÄ‚îÄ Constantes ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

MAX_INSIGHTS = 12          # Plafond d'insights par dream
MIN_SOURCES = 2            # Un insight doit croiser ‚â• 2 sources
SIMILARITY_THRESHOLD = 0.6 # Seuil de d√©tection doublon
STALENESS_DAYS = 7         # Insight plus ancien = moindre poids


# ‚îÄ‚îÄ Data classes ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

@dataclass
class DreamSource:
    """Une source de donn√©es pour le dream."""
    name: str          # ex. "learnings/dev.md"
    kind: str          # learnings | decisions | trace | failure-museum | shared-context
    entries: list[str] = field(default_factory=list)
    dates: list[str] = field(default_factory=list)


@dataclass
class DreamInsight:
    """Un insight √©mergent produit par le dream."""
    title: str
    description: str
    sources: list[str]        # noms des fichiers sources
    category: str             # pattern | tension | opportunity | connection
    confidence: float         # 0.0 - 1.0
    agents_relevant: list[str] = field(default_factory=list)
    actionable: bool = False


# ‚îÄ‚îÄ Collecte des sources ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def collect_sources(project_root: Path, since: str | None = None,
                    agent_filter: str | None = None) -> list[DreamSource]:
    """Collecte toutes les sources de m√©moire du projet."""
    sources: list[DreamSource] = []
    memory_dir = project_root / "_bmad" / "_memory"

    # 1. Learnings
    learnings_dir = memory_dir / "agent-learnings"
    if learnings_dir.exists():
        for f in sorted(learnings_dir.glob("*.md")):
            if agent_filter and agent_filter.lower() not in f.stem.lower():
                continue
            entries = _parse_markdown_entries(f, since)
            if entries:
                sources.append(DreamSource(
                    name=f"learnings/{f.name}",
                    kind="learnings",
                    entries=[e[1] for e in entries],
                    dates=[e[0] for e in entries],
                ))

    # 2. Decisions log
    decisions_file = memory_dir / "decisions-log.md"
    if decisions_file.exists():
        entries = _parse_markdown_entries(decisions_file, since)
        if entries:
            sources.append(DreamSource(
                name="decisions-log.md",
                kind="decisions",
                entries=[e[1] for e in entries],
                dates=[e[0] for e in entries],
            ))

    # 3. BMAD_TRACE
    trace_file = project_root / "_bmad-output" / "BMAD_TRACE.md"
    if trace_file.exists():
        entries = _parse_trace_entries(trace_file, since, agent_filter)
        if entries:
            sources.append(DreamSource(
                name="BMAD_TRACE.md",
                kind="trace",
                entries=[e[1] for e in entries],
                dates=[e[0] for e in entries],
            ))

    # 4. Failure Museum
    failure_file = memory_dir / "failure-museum.md"
    if failure_file.exists():
        entries = _parse_markdown_entries(failure_file, since)
        if entries:
            sources.append(DreamSource(
                name="failure-museum.md",
                kind="failure-museum",
                entries=[e[1] for e in entries],
                dates=[e[0] for e in entries],
            ))

    # 5. Shared context
    shared_file = memory_dir / "shared-context.md"
    if shared_file.exists():
        content = shared_file.read_text(encoding="utf-8")
        sections = _parse_shared_context_sections(content)
        if sections:
            sources.append(DreamSource(
                name="shared-context.md",
                kind="shared-context",
                entries=sections,
            ))

    # 6. Contradiction log
    contradiction_file = memory_dir / "contradiction-log.md"
    if contradiction_file.exists():
        entries = _parse_markdown_entries(contradiction_file, since)
        if entries:
            sources.append(DreamSource(
                name="contradiction-log.md",
                kind="contradictions",
                entries=[e[1] for e in entries],
                dates=[e[0] for e in entries],
            ))

    return sources


def _parse_markdown_entries(path: Path, since: str | None = None) -> list[tuple[str, str]]:
    """Parse un fichier markdown et retourne [(date, text), ...]."""
    try:
        content = path.read_text(encoding="utf-8")
    except OSError:
        return []

    entries: list[tuple[str, str]] = []
    date_pattern = re.compile(r'\[(\d{4}-\d{2}-\d{2})')

    for line in content.splitlines():
        line = line.strip()
        if not line or line.startswith("#"):
            continue
        # Chercher une date dans la ligne
        match = date_pattern.search(line)
        entry_date = match.group(1) if match else ""
        if since and entry_date and entry_date < since:
            continue
        if line.startswith("- ") or line.startswith("* "):
            entries.append((entry_date, line[2:].strip()))

    return entries


def _parse_trace_entries(path: Path, since: str | None = None,
                         agent_filter: str | None = None) -> list[tuple[str, str]]:
    """Parse BMAD_TRACE.md pour les entr√©es pertinentes."""
    try:
        content = path.read_text(encoding="utf-8")
    except OSError:
        return []

    entries: list[tuple[str, str]] = []
    trace_pattern = re.compile(
        r'\[(\d{4}-\d{2}-\d{2})[^\]]*\]\s*\[(\w+)\]\s*\[([^\]]+)\]\s*(.*)'
    )

    for line in content.splitlines():
        match = trace_pattern.match(line.strip())
        if not match:
            continue
        entry_date, level, agent, payload = match.groups()
        if since and entry_date < since:
            continue
        if agent_filter and agent_filter.lower() not in agent.lower():
            continue
        # Focus sur DECISION, CHECKPOINT, FAILURE
        if level in ("DECISION", "CHECKPOINT", "FAILURE", "REMEMBER"):
            entries.append((entry_date, f"[{agent}] [{level}] {payload}"))

    return entries


def _parse_shared_context_sections(content: str) -> list[str]:
    """Extrait les sections non-vides du shared-context."""
    sections: list[str] = []
    current = ""
    for line in content.splitlines():
        if line.startswith("## "):
            if current.strip():
                sections.append(current.strip())
            current = line + "\n"
        else:
            current += line + "\n"
    if current.strip():
        sections.append(current.strip())
    return sections


# ‚îÄ‚îÄ Analyse et g√©n√©ration d'insights ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _extract_keywords(text: str) -> set[str]:
    """Extrait les mots-cl√©s significatifs d'un texte."""
    # Mots vides fran√ßais + anglais
    stopwords = {
        "le", "la", "les", "de", "du", "des", "un", "une", "et", "ou", "en",
        "√†", "au", "aux", "pour", "par", "sur", "dans", "avec", "que", "qui",
        "est", "sont", "a", "ont", "sera", "seront", "pas", "ne", "ni", "mais",
        "the", "an", "is", "are", "was", "were", "be", "been", "being",
        "have", "has", "had", "do", "does", "did", "will", "would", "shall",
        "should", "may", "might", "can", "could", "of", "to", "in", "for",
        "on", "with", "at", "by", "from", "as", "into", "about", "between",
        "after", "before", "not", "no", "but", "or", "and", "if", "then",
        "than", "too", "very", "just", "don", "it", "its", "this", "that",
    }
    words = re.findall(r'[a-zA-Z√Ä-√ø]{3,}', text.lower())
    return {w for w in words if w not in stopwords}


def _similarity(text_a: str, text_b: str) -> float:
    """Similarit√© cosine simplifi√©e par overlap de keywords."""
    ka = _extract_keywords(text_a)
    kb = _extract_keywords(text_b)
    if not ka or not kb:
        return 0.0
    intersection = ka & kb
    union = ka | kb
    return len(intersection) / len(union) if union else 0.0


def find_cross_connections(sources: list[DreamSource]) -> list[DreamInsight]:
    """Trouve les connexions crois√©es entre sources diff√©rentes."""
    insights: list[DreamInsight] = []

    # Comparer chaque paire de sources de types DIFF√âRENTS
    for i, src_a in enumerate(sources):
        for j, src_b in enumerate(sources):
            if j <= i or src_a.kind == src_b.kind:
                continue
            for entry_a in src_a.entries:
                for entry_b in src_b.entries:
                    sim = _similarity(entry_a, entry_b)
                    if sim >= SIMILARITY_THRESHOLD:
                        # Connexion d√©tect√©e !
                        insights.append(DreamInsight(
                            title=f"Connexion {src_a.kind} ‚Üî {src_b.kind}",
                            description=(
                                f"Pattern partag√© entre [{src_a.name}] et [{src_b.name}] :\n"
                                f"  ‚Ä¢ {entry_a[:120]}...\n"
                                f"  ‚Ä¢ {entry_b[:120]}..."
                            ),
                            sources=[src_a.name, src_b.name],
                            category="connection",
                            confidence=round(sim, 2),
                        ))

    return insights


def find_recurring_patterns(sources: list[DreamSource]) -> list[DreamInsight]:
    """D√©tecte les patterns qui reviennent fr√©quemment."""
    insights: list[DreamInsight] = []

    # Compter les keywords globalement
    keyword_freq: dict[str, list[str]] = {}  # keyword ‚Üí [source_names]
    keyword_entries: dict[str, list[str]] = {}  # keyword ‚Üí [entries]

    for src in sources:
        for entry in src.entries:
            keywords = _extract_keywords(entry)
            for kw in keywords:
                keyword_freq.setdefault(kw, []).append(src.name)
                keyword_entries.setdefault(kw, []).append(entry)

    # Trouver les keywords qui apparaissent dans ‚â• MIN_SOURCES sources diff√©rentes
    for kw, src_names in keyword_freq.items():
        unique_sources = list(set(src_names))
        if len(unique_sources) >= MIN_SOURCES and len(src_names) >= 3:
            sample_entries = keyword_entries[kw][:3]
            insights.append(DreamInsight(
                title=f"Pattern r√©current : '{kw}'",
                description=(
                    f"Le terme '{kw}' appara√Æt dans {len(unique_sources)} sources "
                    f"({len(src_names)} occurrences) :\n" +
                    "\n".join(f"  ‚Ä¢ {e[:100]}..." for e in sample_entries)
                ),
                sources=unique_sources,
                category="pattern",
                confidence=min(0.9, 0.3 + 0.1 * len(unique_sources)),
            ))

    return insights


def find_tensions(sources: list[DreamSource]) -> list[DreamInsight]:
    """D√©tecte les tensions et contradictions potentielles."""
    insights: list[DreamInsight] = []

    # Mots indicateurs de tension
    tension_markers = {
        "positive": ["toujours", "always", "must", "doit", "jamais", "never",
                      "obligatoire", "required", "important", "critical"],
        "negative": ["√©viter", "avoid", "ne pas", "never", "jamais", "danger",
                      "risque", "probl√®me", "√©chec", "fail", "broken", "cass√©"],
    }

    positive_entries: list[tuple[str, str]] = []  # (source, entry)
    negative_entries: list[tuple[str, str]] = []

    for src in sources:
        for entry in src.entries:
            entry_lower = entry.lower()
            if any(m in entry_lower for m in tension_markers["positive"]):
                positive_entries.append((src.name, entry))
            if any(m in entry_lower for m in tension_markers["negative"]):
                negative_entries.append((src.name, entry))

    # Croiser positifs et n√©gatifs sur les m√™mes sujets
    for pos_src, pos_entry in positive_entries:
        for neg_src, neg_entry in negative_entries:
            if pos_src == neg_src:
                continue
            sim = _similarity(pos_entry, neg_entry)
            if sim >= 0.3:  # Seuil plus bas pour les tensions
                insights.append(DreamInsight(
                    title=f"Tension d√©tect√©e entre {pos_src} et {neg_src}",
                    description=(
                        f"Possible contradiction sur le m√™me sujet :\n"
                        f"  ‚úÖ [{pos_src}] {pos_entry[:120]}...\n"
                        f"  ‚ùå [{neg_src}] {neg_entry[:120]}..."
                    ),
                    sources=[pos_src, neg_src],
                    category="tension",
                    confidence=round(sim + 0.1, 2),
                ))

    return insights


def find_opportunities(sources: list[DreamSource]) -> list[DreamInsight]:
    """Identifie les opportunit√©s d'am√©lioration non exploit√©es."""
    insights: list[DreamInsight] = []

    # Chercher les patterns "TODO", "√† am√©liorer", "could be better"
    opportunity_markers = [
        "todo", "√† am√©liorer", "could be better", "improvement", "optimiser",
        "refactorer", "simplifier", "automatiser", "manque", "missing",
        "pas encore", "not yet", "futur", "future", "√©ventuellement",
    ]

    for src in sources:
        for entry in src.entries:
            entry_lower = entry.lower()
            for marker in opportunity_markers:
                if marker in entry_lower:
                    insights.append(DreamInsight(
                        title=f"Opportunit√© dans {src.name}",
                        description=f"Signal d'am√©lioration : {entry[:150]}",
                        sources=[src.name],
                        category="opportunity",
                        confidence=0.5,
                        actionable=True,
                    ))
                    break  # Un seul marker suffit par entry

    return insights


# ‚îÄ‚îÄ Validation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def validate_insight(insight: DreamInsight, sources: list[DreamSource]) -> bool:
    """V√©rifie qu'un insight est ancr√© dans les sources (pas d'hallucination)."""
    # R√®gle 1 : doit avoir ‚â• 1 source existante
    if not insight.sources:
        return False

    # R√®gle 2 : les sources r√©f√©renc√©es doivent exister dans la collecte
    source_names = {s.name for s in sources}
    for ref in insight.sources:
        if ref not in source_names:
            return False

    # R√®gle 3 : confiance > 0
    if insight.confidence <= 0:
        return False

    # R√®gle 4 : description non vide
    if not insight.description or len(insight.description) < 10:
        return False

    return True


def deduplicate_insights(insights: list[DreamInsight]) -> list[DreamInsight]:
    """Supprime les insights trop similaires."""
    unique: list[DreamInsight] = []
    for ins in insights:
        is_dupe = False
        for existing in unique:
            if _similarity(ins.description, existing.description) > 0.7:
                # Garder celui avec la meilleure confiance
                if ins.confidence > existing.confidence:
                    unique.remove(existing)
                    unique.append(ins)
                is_dupe = True
                break
        if not is_dupe:
            unique.append(ins)
    return unique


# ‚îÄ‚îÄ Orchestration principale ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def dream(project_root: Path, since: str | None = None,
          agent_filter: str | None = None,
          do_validate: bool = True) -> list[DreamInsight]:
    """Ex√©cute un cycle de dream complet."""

    # 1. Collecte
    sources = collect_sources(project_root, since, agent_filter)
    if not sources:
        return []

    # 2. Analyse multi-dimensionnelle
    all_insights: list[DreamInsight] = []
    all_insights.extend(find_cross_connections(sources))
    all_insights.extend(find_recurring_patterns(sources))
    all_insights.extend(find_tensions(sources))
    all_insights.extend(find_opportunities(sources))

    # 3. Validation
    if do_validate:
        all_insights = [i for i in all_insights if validate_insight(i, sources)]

    # 4. D√©duplication
    all_insights = deduplicate_insights(all_insights)

    # 5. Tri par confiance d√©croissante
    all_insights.sort(key=lambda i: i.confidence, reverse=True)

    # 6. Plafonnement
    return all_insights[:MAX_INSIGHTS]


# ‚îÄ‚îÄ Rendu ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

CATEGORY_ICONS = {
    "connection": "üîó",
    "pattern": "üîÑ",
    "tension": "‚ö°",
    "opportunity": "üí°",
}


def render_journal(insights: list[DreamInsight], sources: list[DreamSource],
                   project_root: Path, since: str | None = None) -> str:
    """G√©n√®re le dream-journal.md en Markdown."""
    now = datetime.now().strftime("%Y-%m-%d %H:%M")
    total_entries = sum(len(s.entries) for s in sources)

    lines = [
        f"# üåô BMAD Dream Journal ‚Äî {now}",
        "",
        f"> Consolidation hors-session ‚Äî {len(sources)} sources, {total_entries} entr√©es analys√©es",
    ]
    if since:
        lines.append(f"> P√©riode : depuis {since}")
    lines.extend(["", "---", ""])

    # R√©sum√© par cat√©gorie
    by_cat: dict[str, list[DreamInsight]] = {}
    for ins in insights:
        by_cat.setdefault(ins.category, []).append(ins)

    lines.append("## üìä R√©sum√©")
    lines.append("")
    lines.append("| Cat√©gorie | Count | Confiance moy. |")
    lines.append("|-----------|-------|----------------|")
    for cat, cat_insights in sorted(by_cat.items()):
        icon = CATEGORY_ICONS.get(cat, "‚ùì")
        avg_conf = sum(i.confidence for i in cat_insights) / len(cat_insights)
        lines.append(f"| {icon} {cat} | {len(cat_insights)} | {avg_conf:.0%} |")
    lines.extend(["", "---", ""])

    # D√©tail des insights
    lines.append("## üß† Insights")
    lines.append("")
    for idx, ins in enumerate(insights, 1):
        icon = CATEGORY_ICONS.get(ins.category, "‚ùì")
        conf_bar = "‚ñà" * int(ins.confidence * 10) + "‚ñë" * (10 - int(ins.confidence * 10))
        lines.append(f"### {icon} {idx}. {ins.title}")
        lines.append("")
        lines.append(f"**Confiance** : `{conf_bar}` {ins.confidence:.0%}")
        lines.append(f"**Sources** : {', '.join(ins.sources)}")
        if ins.actionable:
            lines.append("**üéØ Actionable**")
        lines.append("")
        lines.append(ins.description)
        lines.append("")

    # Sources analys√©es
    lines.extend(["---", "", "## üìö Sources analys√©es", ""])
    for src in sources:
        lines.append(f"- **{src.name}** ({src.kind}) ‚Äî {len(src.entries)} entr√©es")
    lines.append("")

    return "\n".join(lines)


def write_journal(content: str, project_root: Path, dry_run: bool = False) -> Path:
    """√âcrit le journal dans _bmad-output/dream-journal.md."""
    output_dir = project_root / "_bmad-output"
    output_dir.mkdir(parents=True, exist_ok=True)
    journal_path = output_dir / "dream-journal.md"

    if dry_run:
        print(content)
        return journal_path

    # Archiver le journal pr√©c√©dent s'il existe
    if journal_path.exists():
        archive_dir = output_dir / "dream-archives"
        archive_dir.mkdir(exist_ok=True)
        ts = datetime.now().strftime("%Y%m%d-%H%M")
        archive_path = archive_dir / f"dream-journal-{ts}.md"
        journal_path.rename(archive_path)

    journal_path.write_text(content, encoding="utf-8")
    return journal_path


# ‚îÄ‚îÄ CLI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def main():
    parser = argparse.ArgumentParser(
        description="BMAD Dream Mode ‚Äî consolidation hors-session et insights √©mergents",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument("--project-root", default=".", help="Racine du projet BMAD")
    parser.add_argument("--since", default=None, help="Date d√©but (YYYY-MM-DD)")
    parser.add_argument("--agent", default=None, help="Filtrer par agent")
    parser.add_argument("--validate", action="store_true", help="Valider les insights")
    parser.add_argument("--dry-run", action="store_true", help="Afficher sans √©crire")
    parser.add_argument("--json", action="store_true", help="Sortie JSON")

    args = parser.parse_args()
    project_root = Path(args.project_root).resolve()

    # Collecte
    sources = collect_sources(project_root, args.since, args.agent)
    if not sources:
        print("üí§ Aucune source de m√©moire trouv√©e ‚Äî rien √† r√™ver.")
        sys.exit(0)

    total_entries = sum(len(s.entries) for s in sources)
    print(f"üåô Dream Mode ‚Äî {len(sources)} sources, {total_entries} entr√©es")
    print()

    # Dream
    insights = dream(project_root, args.since, args.agent, args.validate)

    if not insights:
        print("üò¥ Aucun insight √©mergent d√©tect√©. Le syst√®me est coh√©rent.")
        sys.exit(0)

    # Sortie JSON
    if args.json:
        data = [
            {
                "title": i.title,
                "description": i.description,
                "sources": i.sources,
                "category": i.category,
                "confidence": i.confidence,
                "actionable": i.actionable,
            }
            for i in insights
        ]
        print(json.dumps(data, indent=2, ensure_ascii=False))
        sys.exit(0)

    # Rendu Markdown
    journal = render_journal(insights, sources, project_root, args.since)
    output_path = write_journal(journal, project_root, args.dry_run)

    if not args.dry_run:
        print(f"‚úÖ {len(insights)} insights √©crits dans {output_path}")
        print()
        # Preview compact
        for idx, ins in enumerate(insights[:5], 1):
            icon = CATEGORY_ICONS.get(ins.category, "‚ùì")
            print(f"  {icon} {idx}. {ins.title} ({ins.confidence:.0%})")
        if len(insights) > 5:
            print(f"  ... et {len(insights) - 5} de plus dans le journal")


if __name__ == "__main__":
    main()
